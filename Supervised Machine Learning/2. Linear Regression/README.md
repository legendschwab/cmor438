Linear regression is a machine learning algorithm that attempts to determine a linear relationship between a set of inputs and the output value. Using the training data, linear regression seeks the estimates of the coefficients that minimizes the least squares error. Note that the "linear" in linear regression refers to linearity of the coefficients, which means that input values can be transformed (e.g. log-transform). 

This is one of the most intuitive and simple algorithms. However, the simplicity means that a linear model may not be appropriate for many data sets. Linear regression assumes that data points are independent, which would not work for something like a time series data. 

In the following notebook, we will apply linear regression to analyze the relationship between [INSERT STUFF HERE]. First, we will perform some diagonistc tests to determine if linear regression is even appropriate and explore different selection of features to determine the best model to use.